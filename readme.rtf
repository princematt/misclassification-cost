{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 CourierNewPSMT;\f1\fmodern\fcharset0 CourierNewPS-BoldMT;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid202\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid203\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid3}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs26 \cf0 Name: kolawole oluwadamilare Matthew\
Student number: N01642906\
\
Documentation of code\
1)\
The data is tab-separated, as indicated by the \\t in the dataset\
I Loaded the datasets with the correct delimiter\
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 \expnd0\expndtw0\kerning0
In the dataset, missing values seem to be represented by a ?. We replaced ? with np.nan for easier handling and then check for missing values\
2)\
The columns ANUMMER_08, ANUMMER_09, and ANUMMER_10 has high percentages of missing values . The next thing I did was to inspect the columns and get the percentage of the missing value to see its significance to the analysis of the data and dropped it\'a0\
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \kerning1\expnd0\expndtw0 Then I converted the DATE_LORDER column to datetime, errors='coerce' will ensure NaN remains NaN\
I went ahead to extract a reference year and month(i.e the maximum data and year in the data set) which is ideal because I want a reducible results and I want to find the numbers of months remaining for the validity of card, age and date since last order \
3)\
Next thing I did for the data preprocessing was to encode the categorical, numeric data and also convert the date/time features in a suitable format for machine learning algorithms I.e(
\f1\b \cb2 \expnd0\expndtw0\kerning0
1, Convert TIME_ORDER to minutes past midnight. 2, Convert ANUMMER_02, ANUMMER_03, ANUMMER_04, ANUMMER_05 to numerical values or perform OneHotEncoding. 3,Treat MAHN_AKT and MAHN_HOECHST as numerical values.
\f0\b0 \cb1 \kerning1\expnd0\expndtw0 )\
\
4)
\f1\b \cb2 \expnd0\expndtw0\kerning0
The next steps I did was use the Logistic Regression Model:\
1,\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Data Preprocessing: Since logistic regression is sensitive to feature scaling, the first step is to scale the data.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 2,\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Model Training: I used the LogisticRegression from sklearn to train the model on the training data.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 3'\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Model Evaluation: l evaluated the model on the validation data.\
\
5)\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 Here are some key takeaways from the classification report:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Precision for 'yes': This is quite low at 0.15. Precision indicates out of all instances predicted as 'yes', only 15% were actually 'yes'. This means that when the model says something is 'yes', there's a good chance it's not.\
Recall for 'yes': This is also quite low at 0.03. Recall indicates out of all actual 'yes' instances, the model could only correctly predict 3% of them. This means the model is missing out on a lot of actual 'yes' cases.\
F1-score for 'yes': Combines precision and recall into a single metric, which is 0.05 for 'yes', confirming that the performance for this class is poor.\
The model has a high accuracy of 0.94 mainly because it's really good at predicting the 'no' class (which appears to be the dominant class given the provided data).\
The imbalanced nature of the dataset (5669 'no' vs 331 'yes') might be contributing to this skewed performance. In imbalanced datasets, machine learning algorithms tend to favour the dominant class.\
\
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \kerning1\expnd0\expndtw0 6)\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 \cb2 \expnd0\expndtw0\kerning0
Next steps that would be done will be:\
1, Predicting Probabilities:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 I began by using the trained logistic regression model to predict the probability estimates for each class on the validation set. then extract the probabilities for the 'yes' (high risk) class.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 2, Defining a Cost Calculation Function:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Next, I defined a function that calculates the misclassification cost based on the provided cost matrix.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 3, Finding the Optimal Threshold:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 I looped through potential threshold values to determine which threshold minimizes the misclassification cost.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 4, Using the Optimal Threshold:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Once I found the optimal threshold, I used it to make predictions on our validation set.\
\
7)\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 I tried using a Gradient Boosted Trees model, specifically the XGBoost implementation to see if its more cost effective. Here's why:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Handling Imbalanced Datasets: XGBoost allows for setting a scale_pos_weight parameter which is useful for imbalanced classes. It provides a balance to the positive class. This can improve recall for the minority class without requiring explicit resampling.\
Flexibility: XGBoost offers a lot of hyperparameters for fine-tuning, which means there's a good chance to find an optimal configuration for this specific dataset.\
Robustness: Gradient boosting, in general, is less prone to overfitting. Given a proper hyperparameter setting, it can generalize well on unseen data.\
Performance: XGBoost often delivers high-performance results and is a staple in many machine learning competitions for classification tasks.\
\
8)\
Based on the results I got from both models:\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 Logistic Regression
\f0\b0 :\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Precision (yes): 0.15\cb1 \
\ls1\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Recall (yes): 0.03\cb1 \
\ls1\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Misclassification Cost: 14455\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 \cb2 XGBoost
\f0\b0 :\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Precision (yes): 0.12\cb1 \
\ls2\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Recall (yes): 0.41\cb1 \
\ls2\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Misclassification Cost: 14665\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 From these metrics:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\b \cf0 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Misclassification Cost
\f0\b0 :\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Logistic Regression has a lower misclassification cost (14455) compared to XGBoost (14665). So, based purely on cost, Logistic Regression is preferable.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\b \cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Precision and Recall for the 'yes' class
\f0\b0 :\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Precision measures the accuracy of positive predictions. Higher precision means that more of the predicted "yes" instances are actual "yes" instances.\cb1 \
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls3\ilvl2\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
Logistic Regression has a higher precision (0.15) for the 'yes' class compared to XGBoost (0.12).\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Recall (or Sensitivity) indicates the fraction of actual 'yes' instances that were captured. A higher recall means that more actual "yes" instances were correctly identified.\cb1 \
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls3\ilvl2\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
XGBoost has a substantially higher recall (0.41) for the 'yes' class compared to Logistic Regression (0.03).\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\b \cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
F1-score
\f0\b0 :\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The F1-score for the 'yes' class, which is the harmonic mean of precision and recall, is higher for XGBoost (0.19) than for Logistic Regression (0.05). A higher F1-score indicates a balance between precision and recall, and in this case, suggests XGBoost is doing a better job at finding a middle ground between the two.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\b \cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	4	}\expnd0\expndtw0\kerning0
Overall Accuracy
\f0\b0 :\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The overall accuracy for Logistic Regression (0.94) is higher than for XGBoost (0.81). However, in imbalanced datasets, accuracy can be misleading, as it might achieve high accuracy by simply predicting the majority class.\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 \cb2 Conclusion
\f0\b0 :\
Given this, the best model would depend on the specific priorities and business objectives. If the priority is to catch as many 'yes' instances as possible (e.g., for preventive measures), even at the expense of having more false positives, XGBoost would be more suited. On the other hand, if the focus is strictly on minimizing the cost, then the slight edge of Logistic Regression in terms of misclassification cost would make it preferable.\

\f1\b The main objective is to minimize the cost of misclassification:\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Misclassifying a high-risk order ('yes') as low-risk ('no') costs 50. Misclassifying a low-risk order ('no') as high-risk ('yes') costs 5. Given this objective, the primary focus is minimizing the misclassification cost so we would go with the logistic regression.\
\
9)\
I did some visualization by computing the confusion matrix to see how many True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) the model produced\
\
10)\
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \kerning1\expnd0\expndtw0 I tested on the test set and, used the saved order_id earlier with the predicted class of yes and no gotten from the test set to save the result on a new file for submission\
}